{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_19256\\3465970873.py:62: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x1=torch.tensor(x)\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Temp\\ipykernel_19256\\3465970873.py:63: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  y1=torch.tensor(y)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/2000 prediction: tensor([ 0.7769,  0.4000,  0.7560,  0.3955,  0.2301,  0.3243,  0.4267,  0.3461,\n",
      "         0.4682,  0.7674,  0.4480,  1.1063,  0.3364,  0.4942,  0.7166, -0.1724,\n",
      "         0.6268,  0.4011,  0.1856, -0.0637]) Cost: 13241.481445\n",
      "Epoch  100/2000 prediction: tensor([ 15.8102, -48.3580,  11.8792, -17.2205, -22.8389, -26.5714,  -2.9239,\n",
      "        -20.0841,  -3.3820,  -2.0718,  53.2360, -22.5857,  -1.7671,  26.6031,\n",
      "        -27.1062, -63.9334,  29.6725,  22.5355, -53.5561,  32.9759]) Cost: 7287.898438\n",
      "Epoch  200/2000 prediction: tensor([  27.5258,  -83.2833,   20.4639,  -30.0407,  -40.0032,  -46.0899,\n",
      "          -5.8382,  -34.5695,   -6.1733,   -3.3442,   91.1921,  -38.0082,\n",
      "          -3.3997,   45.4375,  -46.5401, -110.7373,   51.1588,   38.2671,\n",
      "         -92.8183,   55.5746]) Cost: 4140.033203\n",
      "Epoch  300/2000 prediction: tensor([  36.7662, -108.2559,   27.1179,  -39.4014,  -52.8576,  -60.2634,\n",
      "          -8.4601,  -44.7175,   -8.2109,   -3.5984,  118.5320,  -47.7282,\n",
      "          -4.6362,   59.0489,  -60.0572, -145.1269,   67.1890,   49.4312,\n",
      "        -121.6077,   70.8877]) Cost: 2456.388916\n",
      "Epoch  400/2000 prediction: tensor([  44.1488, -126.0676,   32.2973,  -46.2651,  -62.5592,  -70.5616,\n",
      "         -10.8832,  -51.7027,   -9.7131,   -3.2104,  138.2709,  -53.5464,\n",
      "          -5.5450,   68.9076,  -69.4117, -170.4136,   79.2714,   57.3380,\n",
      "        -142.8132,   81.1372]) Cost: 1540.444580\n",
      "Epoch  500/2000 prediction: tensor([  50.1271, -138.7286,   36.3459,  -51.3252,  -69.9485,  -78.0484,\n",
      "         -13.1666,  -56.3844,  -10.8354,   -2.4358,  152.5658,  -56.7221,\n",
      "          -6.1865,   76.0675,  -75.8464, -189.0152,   88.4891,   62.9226,\n",
      "        -158.5193,   87.8840]) Cost: 1029.651123\n",
      "Epoch  600/2000 prediction: tensor([  55.0352, -147.6857,   39.5234,  -55.0811,  -75.6365,  -83.4939,\n",
      "         -15.3468,  -59.3918,  -11.6889,   -1.4459,  162.9588,  -58.1321,\n",
      "          -6.6129,   81.2844,  -80.2402, -202.6990,   95.6194,   66.8525,\n",
      "        -170.2302,   92.2219]) Cost: 734.704651\n",
      "Epoch  700/2000 prediction: tensor([  59.1209, -153.9809,   42.0272,  -57.8930,  -80.0684,  -87.4564,\n",
      "         -17.4452,  -61.1864,  -12.3521,   -0.3541,  170.5533,  -58.3836,\n",
      "          -6.8683,   85.1006,  -83.2136, -212.7596,  101.2217,   69.6037,\n",
      "        -179.0331,   94.9149]) Cost: 556.322937\n",
      "Epoch  800/2000 prediction: tensor([  62.5680, -158.3646,   44.0077,  -60.0201,  -83.5684,  -90.3406,\n",
      "         -19.4733,  -62.1063,  -12.8810,    0.7666,  176.1381,  -57.8950,\n",
      "          -6.9895,   87.9051,  -85.2032, -220.1462,  105.6983,   71.5160,\n",
      "        -185.7135,   96.4947]) Cost: 442.136658\n",
      "Epoch  900/2000 prediction: tensor([  65.5143, -161.3776,   45.5802,  -61.6497,  -86.3736,  -92.4404,\n",
      "         -21.4369,  -62.4000,  -13.3146,    1.8703,  180.2776,  -56.9518,\n",
      "          -7.0070,   89.9777,  -86.5159, -225.5568,  109.3397,   72.8316,\n",
      "        -190.8404,   97.3301]) Cost: 364.292847\n",
      "Epoch 1000/2000 prediction: tensor([  68.0631, -163.4093,   46.8331,  -62.9166,  -88.6573,  -93.9691,\n",
      "         -23.3380,  -62.2498,  -13.6808,    2.9294,  183.3756,  -55.7473,\n",
      "          -6.9454,   91.5194,  -87.3660, -229.5050,  112.3554,   73.7234,\n",
      "        -194.8256,   97.6751]) Cost: 307.813629\n",
      "Epoch 1100/2000 prediction: tensor([  70.2927, -164.7401,   47.8348,  -63.9180,  -90.5467,  -95.0818,\n",
      "         -25.1766,  -61.7896,  -13.9985,    3.9290,  185.7212,  -54.4106,\n",
      "          -6.8247,   92.6749,  -87.9032, -232.3695,  114.8975,   74.3146,\n",
      "        -197.9681,   97.7040]) Cost: 264.513763\n",
      "Epoch 1200/2000 prediction: tensor([  72.2628, -165.5722,   48.6384,  -64.7242,  -92.1352,  -95.8913,\n",
      "         -26.9519,  -61.1175,  -14.2814,    4.8620,  187.5210,  -53.0262,\n",
      "          -6.6609,   93.5485,  -88.2309, -234.4304,  117.0764,   74.6930,\n",
      "        -200.4848,   97.5351]) Cost: 229.820526\n",
      "Epoch 1300/2000 prediction: tensor([  74.0192, -166.0511,   49.2850,  -65.3858,  -93.4917,  -96.4795,\n",
      "         -28.6628,  -60.3054,  -14.5384,    5.7266,  188.9235,  -51.6483,\n",
      "          -6.4668,   94.2154,  -88.4207, -235.8949,  118.9730,   74.9212,\n",
      "        -202.5337,   97.2485]) Cost: 201.101608\n",
      "Epoch 1400/2000 prediction: tensor([  75.5974, -166.2813,   49.8070,  -65.9395,  -94.6672,  -96.9064,\n",
      "         -30.3082,  -59.4060,  -14.7759,    6.5241,  190.0348,  -50.3104,\n",
      "          -6.2524,   94.7301,  -88.5214, -236.9168,  120.6463,   75.0440,\n",
      "        -204.2302,   96.8970]) Cost: 176.779495\n",
      "Epoch 1500/2000 prediction: tensor([  77.0251, -166.3383,   50.2296,  -66.4118,  -95.6998,  -97.2155,\n",
      "         -31.8874,  -58.4574,  -14.9982,    7.2578,  190.9316,  -49.0320,\n",
      "          -6.0254,   95.1321,  -88.5662, -237.6104,  122.1404,   75.0931,\n",
      "        -205.6585,   96.5156]) Cost: 155.861877\n",
      "Epoch 1600/2000 prediction: tensor([  78.3244, -166.2761,   50.5727,  -66.8218,  -96.6176,  -97.4387,\n",
      "         -33.4001,  -57.4873,  -15.2081,    7.9318,  191.6685,  -47.8232,\n",
      "          -5.7918,   95.4500,  -88.5773, -238.0609,  123.4877,   75.0912,\n",
      "        -206.8805,   96.1266]) Cost: 137.688980\n",
      "Epoch 1700/2000 prediction: tensor([  79.5128, -166.1336,   50.8522,  -67.1837,  -97.4422,  -97.5992,\n",
      "         -34.8465,  -56.5157,  -15.4076,    8.5507,  192.2855,  -46.6882,\n",
      "          -5.5564,   95.7049,  -88.5695, -238.3319,  124.7130,   75.0545,\n",
      "        -207.9417,   95.7439]) Cost: 121.795242\n",
      "Epoch 1800/2000 prediction: tensor([  80.6043, -165.9385,   51.0804,  -67.5074,  -98.1895,  -97.7139,\n",
      "         -36.2271,  -55.5567,  -15.5978,    9.1194,  192.8112,  -45.6273,\n",
      "          -5.3225,   95.9120,  -88.5522, -238.4707,  125.8348,   74.9946,\n",
      "        -208.8759,   95.3757]) Cost: 107.834496\n",
      "Epoch 1900/2000 prediction: tensor([  81.6107, -165.7106,   51.2673,  -67.8006,  -98.8720,  -97.7953,\n",
      "         -37.5428,  -54.6200,  -15.7796,    9.6422,  193.2664,  -44.6385,\n",
      "          -5.0927,   96.0827,  -88.5315, -238.5131,  126.8677,   74.9195,\n",
      "        -209.7082,   95.0264]) Cost: 95.536636\n",
      "Epoch 2000/2000 prediction: tensor([  82.5412, -165.4638,   51.4208,  -68.0686,  -99.4990,  -97.8523,\n",
      "         -38.7950,  -53.7123,  -15.9536,   10.1234,  193.6661,  -43.7182,\n",
      "          -4.8689,   96.2251,  -88.5108, -238.4852,  127.8228,   74.8350,\n",
      "        -210.4572,   94.6981]) Cost: 84.683517\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[93.7705, -1.3601, 20.9094]], requires_grad=True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# 데이터셋\n",
    "x = torch.FloatTensor( [[ 7.71011738e-01, -3.13508197e-01,  5.42352572e-01],\n",
    "                            [-1.85861239e+00,  2.16116006e-01,  5.08396243e-01],\n",
    "                            [ 5.39058321e-01, -1.11792545e+00,  4.15393930e-02],\n",
    "                            [-6.37655012e-01, -2.36184031e-01, -3.38821966e-01],\n",
    "                            [-8.78107893e-01,  9.02525097e-03, -7.47870949e-01],\n",
    "                            [-9.88779049e-01,  2.56570452e-01, -1.56434170e-01],\n",
    "                            [-1.53495196e-01, -1.42121723e+00, -1.18761229e+00],\n",
    "                            [-6.34679305e-01,  1.27837923e+00,  4.33496330e-01],\n",
    "                            [-7.44707629e-02, -3.75669423e-01, -3.81092518e-01],\n",
    "                            [ 9.76147160e-06, -8.44213704e-01,  5.01857207e-01],\n",
    "                            [ 2.29220801e+00,  5.51454045e-01, -9.09007615e-01],\n",
    "                            [-8.41747366e-01, -1.79343559e+00,  1.64027081e+00],\n",
    "                            [ 4.79705919e-02,  6.11340780e-01, -3.35677339e-01],\n",
    "                            [ 1.17500122e+00, -1.91304965e-02, -5.96159700e-01],\n",
    "                            [-1.05795222e+00, -1.24528809e+00,  5.02881417e-01],\n",
    "                            [-2.43476758e+00,  2.23136679e+00, -2.69056960e-01],\n",
    "                            [ 1.35963386e+00,  3.70444537e-01,  1.12726505e-01],\n",
    "                            [ 1.00036589e+00,  8.77102184e-02, -8.29135289e-01],\n",
    "                            [-2.13619610e+00, -5.62668272e-02, -4.16757847e-01],\n",
    "                            [ 1.46767801e+00,  1.73118467e+00, -1.86809065e+00]])\n",
    "\n",
    "y = torch.FloatTensor([[  96.68704955],\n",
    "                            [-158.98008532],\n",
    "                            [  54.49974683],\n",
    "                            [ -71.2832092 ],\n",
    "                            [-109.68370597],\n",
    "                            [ -96.20277494],\n",
    "                            [ -61.52618908],\n",
    "                            [ -38.24501686],\n",
    "                            [ -19.32846993],\n",
    "                            [  16.24620599],\n",
    "                            [ 198.60935931],\n",
    "                            [ -31.37236913],\n",
    "                            [  -0.41438801],\n",
    "                            [  97.7887465 ],\n",
    "                            [ -88.92656277],\n",
    "                            [-234.84110943],\n",
    "                            [ 140.94339104],\n",
    "                            [  72.84533909],\n",
    "                            [-220.9413211 ],\n",
    "                            [  90.02679734]])\n",
    "\n",
    "\n",
    "# 다중 선형 회귀 클래스\n",
    "class MultiLinearRegression(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MultiLinearRegression, self).__init__()\n",
    "        self.linear = nn.Linear(3, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "\n",
    "model = MultiLinearRegression()\n",
    "\n",
    "x1=torch.tensor(x)\n",
    "y1=torch.tensor(y)\n",
    "\n",
    "w1 = torch.zeros(1, requires_grad=True)\n",
    "w2 = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)\n",
    "\n",
    "# 손실 함수와 옵티마이저\n",
    "loss_fn = nn.MSELoss()  # 평균 제곱 오차\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "# 학습\n",
    "epochs = 2000\n",
    "for epoch in range(epochs + 1):\n",
    "    prediction = model(x)\n",
    "\n",
    "    loss = loss_fn(prediction, y)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 ==0:\n",
    "      print('Epoch {:4d}/{} prediction: {} Cost: {:.6f}'.format(\n",
    "          epoch, epochs, prediction.squeeze().detach(), loss.item()\n",
    "      ))\n",
    "    \n",
    "\n",
    "# 예측\n",
    "model.eval()\n",
    "x_test = torch.tensor([[90, 88, 93]], dtype=torch.float32)\n",
    "y_test = model(x_test)\n",
    "#print(f\"예측 값: {y_test.squeeze().detach()}\")\n",
    " \n",
    "model.linear.weight\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([152.2637], grad_fn=<ViewBackward0>)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(torch.Tensor([1,2,3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([-1.5146], requires_grad=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.linear.bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
